from typing import Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_groq import ChatGroq
import os
from dotenv import load_dotenv
from core.models.conversation_models import ConversationState

# Importar funciones directas sin decoradores @tool
import json
import datetime

def search_questions_file_direct(file_path: str = "data/questions.json") -> list[str]:
    """Busca y carga las preguntas desde un archivo local (versi√≥n directa sin @tool)."""
    try:
        if not os.path.exists(file_path):
            default_questions = [
                "¬øCu√°l es tu nombre completo?",
                "¬øCu√°l es tu experiencia laboral previa?",
                "¬øQu√© habilidades t√©cnicas posees?",
                "¬øPor qu√© est√°s interesado en esta posici√≥n?",
                "¬øCu√°les son tus expectativas salariales?"
            ]
            
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump({"questions": default_questions}, f, ensure_ascii=False, indent=2)
            
            return default_questions
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        if isinstance(data, dict) and "questions" in data:
            return data["questions"]
        elif isinstance(data, list):
            return data
        else:
            raise ValueError("Formato de archivo no v√°lido")
            
    except Exception as e:
        print(f"Error al cargar preguntas: {e}")
        return [
            "¬øCu√°l es tu nombre completo?",
            "¬øCu√°l es tu experiencia laboral previa?",
            "¬øQu√© habilidades t√©cnicas posees?"
        ]

def save_user_responses_direct(responses: Dict[str, str], file_path: str = "data/user_responses.json") -> bool:
    """Guarda las respuestas del usuario en un archivo local (versi√≥n directa sin @tool)."""
    try:
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        
        responses["timestamp"] = datetime.datetime.now().isoformat()
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(responses, f, ensure_ascii=False, indent=2)
            
        return True
        
    except Exception as e:
        print(f"Error al guardar respuestas: {e}")
        return False

def simulate_email_send_direct(user_responses: Dict[str, str]) -> bool:
    """Simula el env√≠o de correo (versi√≥n directa sin @tool)."""
    try:
        print("üìß Simulando env√≠o de correo...")
        print(f"Resumen enviado para {len(user_responses)} respuestas")
        return True
    except Exception as e:
        print(f"Error al enviar correo: {e}")
        return False

# Cargar variables de entorno desde .env
load_dotenv()


def initialize_conversation_node(state: ConversationState) -> ConversationState:
    """
    Nodo inicial que carga las preguntas y prepara la conversaci√≥n.
    """
    print("üöÄ Inicializando conversaci√≥n...")
    
    # Cargar preguntas desde archivo
    questions = search_questions_file_direct("data/questions.json")
    state.pending_questions = questions
    state.current_question_index = 0
    
    if questions:
        state.current_question = questions[0]
        
        # Mensaje de bienvenida combinado con la primera pregunta
        welcome_and_question_message = AIMessage(content=f"""¬°Hola! Soy el asistente de RRHH de Adaptiera. 
Voy a realizarte algunas preguntas para conocerte mejor.
Responde con la mayor sinceridad posible.

Empecemos:

{state.current_question}""")
        state.messages.append(welcome_and_question_message)
    
    return state


def process_user_response_node(state: ConversationState) -> ConversationState:
    """
    Nodo que procesa la respuesta del usuario y decide si es satisfactoria.
    """
    print("ü§î Procesando respuesta del usuario...")
    
    # Obtener la √∫ltima respuesta del usuario
    last_message = state.messages[-1] if state.messages else None
    
    if not isinstance(last_message, HumanMessage):
        print("‚ö†Ô∏è No se encontr√≥ respuesta del usuario")
        return state
    
    user_response = last_message.content
    current_question = state.current_question
    
    # Configurar el LLM (Groq)
    groq_api_key = os.getenv("GROQ_API_KEY")
    if not groq_api_key:
        print("‚ö†Ô∏è GROQ_API_KEY no configurada, usando l√≥gica simple")
        # L√≥gica simple sin LLM (m√°s permisiva)
        is_satisfactory = len(user_response.strip()) > 3
        clarification_reason = "Por favor, proporciona una respuesta m√°s detallada" if not is_satisfactory else None
    else:
        # Usar Groq para evaluar la respuesta (modelo actualizado)
        llm = ChatGroq(api_key=groq_api_key, model="llama-3.3-70b-versatile")
        
        evaluation_prompt = f"""
        Eval√∫a si la siguiente respuesta es satisfactoria para la pregunta planteada:
        
        Pregunta: {current_question}
        Respuesta: {user_response}
        
        Responde SOLO con:
        - "SATISFACTORIA" si la respuesta proporciona informaci√≥n b√°sica relevante a la pregunta
        - "NECESITA_CLARIFICACION: [raz√≥n]" si la respuesta est√° completamente vac√≠a, es irrelevante o muy confusa
        
        S√© PERMISIVO en tu evaluaci√≥n. Acepta respuestas que tengan al menos alguna relaci√≥n con la pregunta, 
        incluso si son breves o no muy detalladas. Solo solicita clarificaci√≥n si la respuesta realmente 
        no tiene sentido o est√° completamente fuera de tema.
        """
        
        try:
            evaluation = llm.invoke(evaluation_prompt).content.strip()
            
            if evaluation.startswith("SATISFACTORIA"):
                is_satisfactory = True
                clarification_reason = None
            else:
                is_satisfactory = False
                clarification_reason = evaluation.replace("NECESITA_CLARIFICACION:", "").strip()
        except Exception as e:
            print(f"Error al evaluar con Groq: {e}")
            # Fallback a l√≥gica simple (m√°s permisiva)
            is_satisfactory = len(user_response.strip()) > 3
            clarification_reason = "Por favor, proporciona una respuesta m√°s detallada" if not is_satisfactory else None
    
    # Actualizar estado
    if is_satisfactory:
        # Guardar respuesta satisfactoria
        state.user_responses[current_question] = user_response
        save_user_responses_direct(state.user_responses, "data/user_responses.json")
        state.needs_clarification = False
        state.clarification_reason = None
        print(f"‚úÖ Respuesta aceptada para: {current_question}")
    else:
        state.needs_clarification = True
        state.clarification_reason = clarification_reason
        print(f"‚ùì Necesita clarificaci√≥n: {clarification_reason}")
    
    return state


def clarification_node(state: ConversationState) -> ConversationState:
    """
    Nodo que solicita aclaraci√≥n cuando la respuesta no es satisfactoria.
    """
    print("üîÑ Solicitando aclaraci√≥n...")
    
    clarification_message = AIMessage(content=f"""
Me gustar√≠a que puedas ampliar tu respuesta anterior.
{state.clarification_reason}

Por favor, proporciona m√°s detalles sobre: {state.current_question}
""")
    
    state.messages.append(clarification_message)
    return state


def next_question_node(state: ConversationState) -> ConversationState:
    """
    Nodo que avanza a la siguiente pregunta.
    """
    print("‚û°Ô∏è Avanzando a la siguiente pregunta...")
    
    state.current_question_index += 1
    
    if state.current_question_index < len(state.pending_questions):
        # Hay m√°s preguntas
        state.current_question = state.pending_questions[state.current_question_index]
        
        next_question_message = AIMessage(content=f"""
Perfecto, gracias por tu respuesta.

Siguiente pregunta:
{state.current_question}
""")
        state.messages.append(next_question_message)
    else:
        # No hay m√°s preguntas, marcar como completa
        state.conversation_complete = True
        state.current_question = None
        
        completion_message = AIMessage(content="""
¬°Excelente! Hemos terminado con todas las preguntas.
Ahora voy a procesar tu informaci√≥n y enviar un resumen.
""")
        state.messages.append(completion_message)
    
    return state


def finalize_conversation_node(state: ConversationState) -> ConversationState:
    """
    Nodo final que guarda las respuestas y env√≠a el correo.
    """
    print("üèÅ Finalizando conversaci√≥n...")
    
    # Enviar correo (simulado por ahora)
    email_success = simulate_email_send_direct(state.user_responses)
    
    if email_success:
        final_message = AIMessage(content="""
¬°Muchas gracias por tu tiempo! 

‚úÖ Tus respuestas han sido guardadas correctamente
‚úÖ Se ha enviado un resumen por correo electr√≥nico

Nuestro equipo de RRHH revisar√° tu informaci√≥n y se pondr√° en contacto contigo pronto.

¬°Que tengas un excelente d√≠a!
""")
    else:
        final_message = AIMessage(content="""
Gracias por completar la entrevista. 
Hubo algunos problemas t√©cnicos al procesar tu informaci√≥n, 
pero nuestro equipo se pondr√° en contacto contigo pronto.
""")
    
    state.messages.append(final_message)
    return state


def decision_node(state: ConversationState) -> str:
    """
    Funci√≥n de decisi√≥n que determina el siguiente paso en el flujo.
    Esta funci√≥n NO modifica el estado, solo retorna la decisi√≥n.
    """
    print("üéØ Tomando decisi√≥n sobre el siguiente paso...")
    print(f"   Estado actual: conversation_complete={state.conversation_complete}, needs_clarification={state.needs_clarification}")
    print(f"   Pregunta actual: {state.current_question}")
    print(f"   √çndice: {state.current_question_index}, Total preguntas: {len(state.pending_questions)}")
    print(f"   √öltimo mensaje: {type(state.messages[-1]).__name__ if state.messages else 'Ninguno'}")
    
    # Si la conversaci√≥n est√° completa, ir al nodo final
    if state.conversation_complete:
        print("   ‚Üí Conversaci√≥n completa, finalizando...")
        return "finalize"
    
    # Si necesita aclaraci√≥n, ir al nodo de aclaraci√≥n
    if state.needs_clarification:
        print("   ‚Üí Necesita aclaraci√≥n...")
        return "clarify"
    
    # Si hay un mensaje del usuario pendiente de procesar
    if state.messages and isinstance(state.messages[-1], HumanMessage):
        print("   ‚Üí Procesando respuesta del usuario...")
        return "process_response"
    
    # Si acabamos de procesar una respuesta satisfactoria, avanzar a la siguiente pregunta
    # Esto se detecta cuando: no necesita aclaraci√≥n Y tenemos una pregunta actual Y hay respuestas guardadas
    if (not state.needs_clarification and 
        state.current_question and 
        state.current_question in state.user_responses and
        state.messages and 
        isinstance(state.messages[-1], AIMessage)):
        print("   ‚Üí Respuesta procesada exitosamente, avanzando a siguiente pregunta...")
        return "next_question"
    
    # Si no hay pregunta actual pero no est√° completa, ir a la siguiente
    if not state.current_question and not state.conversation_complete:
        print("   ‚Üí No hay pregunta actual, avanzando a siguiente pregunta...")
        return "next_question"
    
    # Si tenemos una pregunta actual pero es la inicial (sin respuestas del usuario a√∫n)
    if (state.current_question and 
        len(state.user_responses) == 0 and 
        state.messages and 
        isinstance(state.messages[-1], AIMessage)):
        print("   ‚Üí Estado inicial con pregunta lista, esperando respuesta del usuario...")
        return "wait_for_user"
    
    # Por defecto, esperar respuesta del usuario
    print("   ‚Üí Esperando respuesta del usuario...")
    return "wait_for_user"


def dummy_decision_node(state: ConversationState) -> ConversationState:
    """
    Nodo dummy que no hace nada, solo para mantener la estructura del grafo.
    La l√≥gica real est√° en la funci√≥n decision_node que se usa para routing.
    """
    return state 